{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import geolib.geohash\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from cri98tj.partitioners.Geohash_partitioner import Geohash_partitioner\n",
    "from cri98tj.normalizers.FirstPoint_normalizer import FirstPoint_normalizer\n",
    "from cri98tj.selectors.RandomInformationGain_selector import RandomInformationGain_selector\n",
    "from cri98tj.distancers.Euclidean_distancer import Euclidean_distancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cri98tj.distancers.Euclidean_distancer import euclideanBestFitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_original = pd.read_csv('../examples/Animals Dataset/data/animals_preapred.zip').sort_values(by=[\"tid\", \"t\"])\n",
    "df_original = df_original[[\"tid\", \"class\", \"c1\", \"c2\", \"t\"]]\n",
    "df_original.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tid_train, tid_test, _, _ = train_test_split(df_original.groupby(by=[\"tid\"]).max().reset_index()[\"tid\"],\n",
    "                                                        df_original.groupby(by=[\"tid\"]).max().reset_index()[\"class\"],\n",
    "                                                        test_size=.3,\n",
    "                                                        stratify=df_original.groupby(by=[\"tid\"]).max().reset_index()[\"class\"],\n",
    "                                                        random_state=3)\n",
    "\n",
    "spatioTemporalCols = [\"c1\", \"c2\", \"t\"]\n",
    "n_movelets=50\n",
    "n_jobs = 24\n",
    "verbose = False\n",
    "precision = 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "\n",
    "def compute_measures(test, pred):\n",
    "    return (accuracy_score(test, pred), precision_score(test, pred, average=\"micro\"), f1_score(test, pred, average=\"micro\"), recall_score(test, pred, average=\"micro\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def myconcat_np(big, small):\n",
    "    if len(big[0]) < len(small[0]):\n",
    "        return myconcat_np(small, big)\n",
    "\n",
    "    resized_small = np.empty((len(small), len(big[0])))\n",
    "\n",
    "    resized_small[:] = np.nan\n",
    "\n",
    "    big_offset = int(len(big[0])/3)\n",
    "    small_offset = int(len(small[0])/3)\n",
    "\n",
    "    for i, row in enumerate(small):\n",
    "        for k in range(3):\n",
    "            for j in range(small_offset):\n",
    "                resized_small[i, k*big_offset+j] = row[k*small_offset+j]\n",
    "\n",
    "    return np.concatenate((big, resized_small))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from cri98tj.selectors.selector_utils import select_and_pivot\n",
    "\n",
    "\n",
    "def my_transform(self, X):\n",
    "    self._checkFormat(X)\n",
    "\n",
    "    df = pd.DataFrame(X.copy(), columns=[\"tid\", \"class\"] + self.spatioTemporalColumns + [\"partId\"])\n",
    "    n_class = len(df[\"class\"].unique())\n",
    "\n",
    "    if self.n_movelets is None:\n",
    "        self.n_movelets = len(df.partId.unique())  # upper bound\n",
    "    elif self.n_movelets < 1:\n",
    "        self.n_movelets = round(self.n_movelets * len(df.partId.unique()))\n",
    "\n",
    "\n",
    "    movelets_to_test1 = select_and_pivot(df, self.n_movelets/n_class, self.normalizer).values[:, 1:]\n",
    "\n",
    "    #df.partId = df.partId.apply(lambda x: x[:x.index(\"_\")-1]+x[x.index(\"_\"):])\n",
    "    new_gh = []\n",
    "    prec=\"|\"\n",
    "    prec_tid= -1\n",
    "    c=-1\n",
    "    for row in df.values:\n",
    "        gh = row[-1]\n",
    "        gh = gh[:gh.index(\"_\")-1]\n",
    "        if gh != prec or row[0] != prec_tid:\n",
    "            prec_tid = row[0]\n",
    "            prec = gh\n",
    "            c+=1\n",
    "        new_gh.append(gh+\"_\"+str(c))\n",
    "\n",
    "    df.partId = new_gh\n",
    "\n",
    "    movelets_to_test2 = select_and_pivot(df, self.n_movelets/n_class, self.normalizer).values[:, 1:]\n",
    "\n",
    "    movelets_to_test = myconcat_np(movelets_to_test1, movelets_to_test2)\n",
    "\n",
    "    df.partId = df.tid\n",
    "\n",
    "    if self.n_trajectories is None:\n",
    "        self.n_trajectories = len(df.partId.unique())  # upper bound\n",
    "    elif self.n_trajectories < 1:\n",
    "        self.n_trajectories = round(self.n_trajectories * len(df.partId.unique()))\n",
    "    trajectories_for_orderline_df = select_and_pivot(df, self.n_trajectories/n_class, self.normalizer)\n",
    "    trajectories_for_orderline = trajectories_for_orderline_df.drop(columns=[\"class\"]).values\n",
    "    y_trajectories_for_orderline = trajectories_for_orderline_df[[\"class\"]].values\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    if self.verbose: print(F\"Computing scores\")\n",
    "\n",
    "    dist_matrix = np.zeros((len(trajectories_for_orderline), len(movelets_to_test)))\n",
    "\n",
    "    executor = ProcessPoolExecutor(max_workers=self.n_jobs)\n",
    "    processes = []\n",
    "    for movelet in tqdm(movelets_to_test, disable=not self.verbose, position=0, leave=True):\n",
    "        processes.append(executor.submit(self._computeDist, trajectories_for_orderline, movelet))\n",
    "\n",
    "    for i, process in enumerate(tqdm(processes)):\n",
    "        res = process.result()\n",
    "        for j, el in enumerate(res):\n",
    "            dist_matrix[j][i] = el\n",
    "\n",
    "    movelets = []\n",
    "    for i, (score, movelet) in enumerate(sorted(zip(scores, movelets_to_test), key=lambda x: x[0], reverse=True)):\n",
    "        if i >= self.top_k: break\n",
    "\n",
    "        if self.verbose: print(F\"{i}.\\t score={score}\")\n",
    "\n",
    "    mutualInfo = mutual_info_classif(dist_matrix, y_trajectories_for_orderline, n_neighbors=self.n_neighbors,\n",
    "                                     random_state=self.random_state)\n",
    "\n",
    "    for i, (score, mov) in enumerate(sorted(zip(mutualInfo, movelets_to_test), key=lambda x: x[0], reverse=True)):\n",
    "        if i >= self.top_k: break\n",
    "        movelets.append(mov)\n",
    "        if self.verbose: print(F\"{i}.\\t score={score}\")\n",
    "\n",
    "    return movelets  # list of list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from cri98tj.distancers.InterpolatedRootDistance_distancer import InterpolatedRootDistance_distancer, \\\n",
    "    InterpolatedRootDistanceBestFitting\n",
    "\n",
    "precision = 3\n",
    "\n",
    "res = []\n",
    "n_mov_rig = []\n",
    "time = []\n",
    "for i in tqdm(range(2, 20)):\n",
    "    df = df_original[[\"tid\", \"class\", \"c1\", \"c2\", \"t\"]].copy()\n",
    "    \n",
    "    res.append((.0, .0, .0, .0, .0))\n",
    "    n_mov_rig.append(0)\n",
    "    time.append(.0)\n",
    "\n",
    "    for _ in range(3):\n",
    "        normalizer = FirstPoint_normalizer(spatioTemporalColumns=spatioTemporalCols, fillna=None, verbose=verbose)\n",
    "        distancer = InterpolatedRootDistance_distancer(normalizer=normalizer, spatioTemporalColumns=spatioTemporalCols, n_jobs=n_jobs, verbose=verbose)\n",
    "        partitioner = Geohash_partitioner(precision=precision, spatioTemporalColumns=spatioTemporalCols, verbose=verbose)\n",
    "\n",
    "        start = datetime.now()\n",
    "        part = partitioner.fit_transform(df[df.tid.isin(tid_train)].values)\n",
    "        selector = RandomInformationGain_selector(top_k=int(1.4**i), bestFittingMeasure=InterpolatedRootDistanceBestFitting, movelets_per_class=None, trajectories_for_orderline=None, n_jobs=n_jobs, spatioTemporalColumns=spatioTemporalCols, normalizer=normalizer, verbose=verbose)\n",
    "        shapelets = my_transform(selector, part)\n",
    "        _, dist_np = distancer.fit_transform((df.values, shapelets))\n",
    "        stop = start - datetime.now()\n",
    "\n",
    "        n_mov_rig[i-2] += (dist_np.shape[1])\n",
    "\n",
    "        clf = RandomForestClassifier(max_depth=3, random_state=3, n_jobs=n_jobs, n_estimators=5000)\n",
    "\n",
    "        dist_np_df = pd.DataFrame(dist_np)\n",
    "        X = dist_np_df.drop(columns=[0]).values\n",
    "        y = dist_np_df[0].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=3)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        res[i-2] = tuple(a+b for a, b in zip(compute_measures(y_test, y_pred), res[i-2]))\n",
    "        time[i-2] += stop.total_seconds()*1000 #millisecondi\n",
    "\n",
    "    res[i-2] = list(map(lambda x: x/3, res[i-2]))\n",
    "    n_mov_rig[i-2] /= 3\n",
    "    time[i-2] /= 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res_rig = pd.DataFrame(res, columns=[\"acc\", \"prec\", \"f1\", \"recall\"])\n",
    "\n",
    "df_res_rig[\"t\"] = time\n",
    "\n",
    "df_res_rig.t *= -1\n",
    "\n",
    "df_res_rig[\"n\"] = n_mov_rig\n",
    "\n",
    "df_res_rig.to_csv(f\"Test n_movelet animals RIG {precision-1}-{precision}.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## da qui eliminare"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score\n",
    "from cri98tj.distancers.InterpolatedRootDistance_distancer import InterpolatedRootDistance_distancer, \\\n",
    "    InterpolatedRootDistanceBestFitting\n",
    "\n",
    "precision = 4\n",
    "\n",
    "res = []\n",
    "n_mov_rig = []\n",
    "time = []\n",
    "for i in tqdm(range(2, 20)):\n",
    "    df = df_original[[\"tid\", \"class\", \"c1\", \"c2\", \"t\"]].copy()\n",
    "    \n",
    "    res.append((.0, .0, .0, .0, .0))\n",
    "    n_mov_rig.append(0)\n",
    "    time.append(.0)\n",
    "\n",
    "    for _ in range(3):\n",
    "        normalizer = FirstPoint_normalizer(spatioTemporalColumns=spatioTemporalCols, fillna=None, verbose=verbose)\n",
    "        distancer = InterpolatedRootDistance_distancer(normalizer=normalizer, spatioTemporalColumns=spatioTemporalCols, n_jobs=n_jobs, verbose=verbose)\n",
    "        partitioner = Geohash_partitioner(precision=precision, spatioTemporalColumns=spatioTemporalCols, verbose=verbose)\n",
    "\n",
    "        start = datetime.now()\n",
    "        part = partitioner.fit_transform(df[df.tid.isin(tid_train)].values)\n",
    "        selector = RandomInformationGain_selector(top_k=int(1.4**i), bestFittingMeasure=InterpolatedRootDistanceBestFitting, movelets_per_class=None, trajectories_for_orderline=None, n_jobs=n_jobs, spatioTemporalColumns=spatioTemporalCols, normalizer=normalizer, verbose=verbose)\n",
    "        shapelets = my_transform(selector, part)\n",
    "        _, dist_np = distancer.fit_transform((df.values, shapelets))\n",
    "        stop = start - datetime.now()\n",
    "\n",
    "        n_mov_rig[i-2] += (dist_np.shape[1])\n",
    "\n",
    "        clf = RandomForestClassifier(max_depth=3, random_state=3, n_jobs=n_jobs, n_estimators=5000)\n",
    "\n",
    "        dist_np_df = pd.DataFrame(dist_np)\n",
    "        X = dist_np_df.drop(columns=[0]).values\n",
    "        y = dist_np_df[0].values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, stratify=y, random_state=3)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        res[i-2] = tuple(a+b for a, b in zip(compute_measures(y_test, y_pred), res[i-2]))\n",
    "        time[i-2] += stop.total_seconds()*1000 #millisecondi\n",
    "\n",
    "    res[i-2] = list(map(lambda x: x/3, res[i-2]))\n",
    "    n_mov_rig[i-2] /= 3\n",
    "    time[i-2] /= 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_res_rig = pd.DataFrame(res, columns=[\"acc\", \"prec\", \"f1\", \"recall\"])\n",
    "\n",
    "df_res_rig[\"t\"] = time\n",
    "\n",
    "df_res_rig.t *= -1\n",
    "\n",
    "df_res_rig[\"n\"] = n_mov_rig\n",
    "\n",
    "df_res_rig.to_csv(f\"Test n_movelet animals RIG {precision-1}-{precision}.csv\", index=None)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}